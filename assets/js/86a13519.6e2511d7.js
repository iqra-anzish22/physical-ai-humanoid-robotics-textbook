"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[589],{53:(e,n,i)=>{function t(e){var n,i,a="";if("string"==typeof e||"number"==typeof e)a+=e;else if("object"==typeof e)if(Array.isArray(e))for(n=0;n<e.length;n++)e[n]&&(i=t(e[n]))&&(a&&(a+=" "),a+=i);else for(n in e)e[n]&&(a&&(a+=" "),a+=n);return a}i.d(n,{A:()=>a});const a=function(){for(var e,n,i=0,a="";i<arguments.length;)(e=arguments[i++])&&(n=t(e))&&(a&&(a+=" "),a+=n);return a}},2995:(e,n,i)=>{i.r(n),i.d(n,{contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>c,toc:()=>l});var t=i(8168),a=(i(6540),i(5680)),s=i(9591);const r={title:"VLA Exercises",sidebar_position:3,description:"Hands-on exercises to reinforce Vision-Language-Action concepts",keywords:["vla","exercises","robotics","ai","vision-language-action"]},o="VLA Exercises",c={unversionedId:"vla-basics/exercises",id:"vla-basics/exercises",isDocsHomePage:!1,title:"VLA Exercises",description:"Hands-on exercises to reinforce Vision-Language-Action concepts",source:"@site/docs/vla-basics/exercises.md",sourceDirName:"vla-basics",slug:"/vla-basics/exercises",permalink:"/physical-ai-humanoid-robotics-textbook/vla-basics/exercises",editUrl:"https://github.com/iqra-anzish22/physical-ai-humanoid-robotics-textbook/edit/main/docs/vla-basics/exercises.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"VLA Exercises",sidebar_position:3,description:"Hands-on exercises to reinforce Vision-Language-Action concepts",keywords:["vla","exercises","robotics","ai","vision-language-action"]},sidebar:"textbookSidebar",previous:{title:"VLA Applications and Use Cases",permalink:"/physical-ai-humanoid-robotics-textbook/vla-basics/applications"},next:{title:"Creating Content for Physical AI & Robotics Textbook",permalink:"/physical-ai-humanoid-robotics-textbook/tutorial-basics/creating-content"}},l=[{value:"Exercise 1: VLA Architecture Analysis",id:"exercise-1-vla-architecture-analysis",children:[]},{value:"Exercise 2: Language Instruction Processing",id:"exercise-2-language-instruction-processing",children:[]},{value:"Exercise 3: Visual Grounding Implementation",id:"exercise-3-visual-grounding-implementation",children:[]},{value:"Exercise 4: VLA Training Pipeline",id:"exercise-4-vla-training-pipeline",children:[]},{value:"Exercise 5: Safety-Critical VLA Design",id:"exercise-5-safety-critical-vla-design",children:[]},{value:"Exercise 6: Multi-Modal Fusion",id:"exercise-6-multi-modal-fusion",children:[]},{value:"Exercise 7: Zero-Shot Task Generalization",id:"exercise-7-zero-shot-task-generalization",children:[]},{value:"Exercise 8: Human-Robot Interaction Design",id:"exercise-8-human-robot-interaction-design",children:[]},{value:"Exercise 9: VLA Performance Evaluation",id:"exercise-9-vla-performance-evaluation",children:[]},{value:"Exercise 10: Real-Time VLA Implementation",id:"exercise-10-real-time-vla-implementation",children:[]},{value:"Exercise 11: VLA for Household Tasks",id:"exercise-11-vla-for-household-tasks",children:[]},{value:"Exercise 12: Imitation Learning for VLA",id:"exercise-12-imitation-learning-for-vla",children:[]},{value:"Exercise 13: Reinforcement Learning Integration",id:"exercise-13-reinforcement-learning-integration",children:[]},{value:"Exercise 14: Multi-Robot VLA Coordination",id:"exercise-14-multi-robot-vla-coordination",children:[]},{value:"Exercise 15: VLA Ethical Considerations",id:"exercise-15-vla-ethical-considerations",children:[]},{value:"Exercise 16: VLA Simulation to Reality Transfer",id:"exercise-16-vla-simulation-to-reality-transfer",children:[]},{value:"Exercise 17: VLA for Assistive Robotics",id:"exercise-17-vla-for-assistive-robotics",children:[]},{value:"Exercise 18: VLA Hardware Integration",id:"exercise-18-vla-hardware-integration",children:[]},{value:"Exercise 19: VLA in Dynamic Environments",id:"exercise-19-vla-in-dynamic-environments",children:[]},{value:"Exercise 20: VLA Explainability",id:"exercise-20-vla-explainability",children:[]},{value:"Programming Exercise 1: Simple VLA Pipeline",id:"programming-exercise-1-simple-vla-pipeline",children:[]},{value:"Programming Exercise 2: Visual Grounding Implementation",id:"programming-exercise-2-visual-grounding-implementation",children:[]},{value:"Project Exercise: VLA System for Simple Manipulation",id:"project-exercise-vla-system-for-simple-manipulation",children:[]},{value:"Research Exercise: VLA Architecture Innovation",id:"research-exercise-vla-architecture-innovation",children:[]},{value:"Summary",id:"summary",children:[]}],d={toc:l},m="wrapper";function u({components:e,...n}){return(0,a.yg)(m,(0,t.A)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"vla-exercises"},"VLA Exercises"),(0,a.yg)("h2",{id:"exercise-1-vla-architecture-analysis"},"Exercise 1: VLA Architecture Analysis"),(0,a.yg)(s.A,{title:"VLA Model Architecture Comparison",difficulty:"intermediate",type:"theoretical",instructions:"Compare and contrast different VLA architectures including RT-1, RT-2, and other prominent models. Analyze their strengths, weaknesses, and appropriate use cases. Create a detailed comparison table showing model capacity, training data requirements, and performance characteristics.",expectedOutcome:"Students will understand different VLA architectures and be able to select appropriate architectures for specific applications.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-2-language-instruction-processing"},"Exercise 2: Language Instruction Processing"),(0,a.yg)(s.A,{title:"Natural Language Understanding for Robotics",difficulty:"intermediate",type:"practical",instructions:"Implement a system that can parse natural language instructions and convert them into structured robot commands. Consider different types of instructions (imperative, conditional, temporal) and their appropriate robotic interpretations.",expectedOutcome:"Students will understand the challenges in language-to-action conversion and implement basic instruction parsing systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-3-visual-grounding-implementation"},"Exercise 3: Visual Grounding Implementation"),(0,a.yg)(s.A,{title:"Visual Grounding for Object Recognition",difficulty:"advanced",type:"practical",instructions:"Create a visual grounding system that can identify objects in an image based on natural language descriptions. Implement attention mechanisms that highlight relevant visual regions based on language input.",expectedOutcome:"Students will understand visual grounding techniques and be able to implement systems that connect language to visual elements.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-4-vla-training-pipeline"},"Exercise 4: VLA Training Pipeline"),(0,a.yg)(s.A,{title:"VLA Dataset Creation and Training",difficulty:"advanced",type:"project",instructions:"Design and implement a complete VLA training pipeline including data collection, preprocessing, model training, and evaluation. Use a simulation environment to generate training data and train a simple VLA model for basic manipulation tasks.",expectedOutcome:"Students will understand the complete VLA development process and be able to create their own VLA training systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-5-safety-critical-vla-design"},"Exercise 5: Safety-Critical VLA Design"),(0,a.yg)(s.A,{title:"Safe VLA System Design",difficulty:"advanced",type:"theoretical",instructions:"Design a VLA system for a safety-critical application (e.g., healthcare, industrial automation). Include safety mechanisms, fail-safe behaviors, and human oversight capabilities. Analyze potential failure modes and mitigation strategies.",expectedOutcome:"Students will understand safety considerations in VLA systems and be able to design safe VLA implementations.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-6-multi-modal-fusion"},"Exercise 6: Multi-Modal Fusion"),(0,a.yg)(s.A,{title:"Vision-Language-Action Fusion Strategies",difficulty:"advanced",type:"practical",instructions:"Implement different multi-modal fusion strategies (early fusion, late fusion, cross-attention) for combining visual, language, and action information. Compare their performance on a simple manipulation task.",expectedOutcome:"Students will understand different multi-modal fusion techniques and their trade-offs for VLA systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-7-zero-shot-task-generalization"},"Exercise 7: Zero-Shot Task Generalization"),(0,a.yg)(s.A,{title:"Zero-Shot Task Generalization in VLA",difficulty:"advanced",type:"simulation",instructions:"Evaluate the zero-shot generalization capabilities of a VLA model by testing it on novel tasks that weren't in the training data. Analyze the factors that contribute to successful generalization and identify limitations.",expectedOutcome:"Students will understand zero-shot learning in VLA systems and be able to evaluate generalization capabilities.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-8-human-robot-interaction-design"},"Exercise 8: Human-Robot Interaction Design"),(0,a.yg)(s.A,{title:"Natural Human-Robot Interaction with VLA",difficulty:"intermediate",type:"project",instructions:"Design and implement a human-robot interaction system that allows natural communication using VLA capabilities. Include mechanisms for clarification, error recovery, and collaborative task execution.",expectedOutcome:"Students will understand human-robot interaction principles and be able to implement natural interaction systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-9-vla-performance-evaluation"},"Exercise 9: VLA Performance Evaluation"),(0,a.yg)(s.A,{title:"Comprehensive VLA Evaluation Framework",difficulty:"advanced",type:"theoretical",instructions:"Design a comprehensive evaluation framework for VLA systems that includes metrics for perception accuracy, language understanding, action success, safety, and user satisfaction. Implement the evaluation on a simple VLA system.",expectedOutcome:"Students will understand VLA evaluation methodologies and be able to design comprehensive evaluation frameworks.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-10-real-time-vla-implementation"},"Exercise 10: Real-Time VLA Implementation"),(0,a.yg)(s.A,{title:"Real-Time VLA System Optimization",difficulty:"advanced",type:"practical",instructions:"Implement optimization techniques for real-time VLA operation including model compression, efficient inference, and pipeline optimization. Measure latency, throughput, and accuracy trade-offs.",expectedOutcome:"Students will understand real-time optimization for VLA systems and be able to implement efficient VLA deployments.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-11-vla-for-household-tasks"},"Exercise 11: VLA for Household Tasks"),(0,a.yg)(s.A,{title:"Household Task VLA Implementation",difficulty:"advanced",type:"simulation",instructions:"Create a VLA system specifically designed for household tasks. Implement recognition of common household objects, understanding of domestic language, and safe manipulation strategies for home environments.",expectedOutcome:"Students will understand domain-specific VLA implementation and be able to adapt VLA systems for specific applications.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-12-imitation-learning-for-vla"},"Exercise 12: Imitation Learning for VLA"),(0,a.yg)(s.A,{title:"Imitation Learning in VLA Systems",difficulty:"advanced",type:"practical",instructions:"Implement an imitation learning approach for training VLA systems using human demonstrations. Include techniques for learning from observation and correcting for distribution shift between demonstration and execution.",expectedOutcome:"Students will understand imitation learning for VLA and be able to implement demonstration-based training systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-13-reinforcement-learning-integration"},"Exercise 13: Reinforcement Learning Integration"),(0,a.yg)(s.A,{title:"Reinforcement Learning for VLA Improvement",difficulty:"advanced",type:"simulation",instructions:"Integrate reinforcement learning with a pre-trained VLA system to improve task performance. Design appropriate reward functions and exploration strategies for VLA systems.",expectedOutcome:"Students will understand how to combine supervised learning with reinforcement learning in VLA systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-14-multi-robot-vla-coordination"},"Exercise 14: Multi-Robot VLA Coordination"),(0,a.yg)(s.A,{title:"Coordinated Multi-Robot VLA Systems",difficulty:"advanced",type:"simulation",instructions:"Design and implement a system where multiple VLA-enabled robots coordinate to complete complex tasks. Include communication protocols, task allocation, and conflict resolution mechanisms.",expectedOutcome:"Students will understand multi-agent VLA coordination and be able to implement collaborative robotic systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-15-vla-ethical-considerations"},"Exercise 15: VLA Ethical Considerations"),(0,a.yg)(s.A,{title:"Ethical Framework for VLA Deployment",difficulty:"intermediate",type:"theoretical",instructions:"Analyze the ethical implications of deploying VLA systems in various domains. Create an ethical framework that addresses privacy, safety, job displacement, and human agency concerns. Propose guidelines for responsible VLA development and deployment.",expectedOutcome:"Students will understand ethical considerations in VLA systems and be able to propose responsible development practices.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-16-vla-simulation-to-reality-transfer"},"Exercise 16: VLA Simulation to Reality Transfer"),(0,a.yg)(s.A,{title:"Sim-to-Real Transfer for VLA Systems",difficulty:"advanced",type:"project",instructions:"Implement techniques for transferring VLA policies trained in simulation to real robots. Include domain randomization, system identification, and adaptation strategies to minimize the sim-to-real gap.",expectedOutcome:"Students will understand sim-to-real transfer challenges and be able to implement effective transfer strategies.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-17-vla-for-assistive-robotics"},"Exercise 17: VLA for Assistive Robotics"),(0,a.yg)(s.A,{title:"Assistive Robotics with VLA",difficulty:"advanced",type:"theoretical",instructions:"Design a VLA system for assistive robotics applications. Consider user needs, accessibility requirements, safety considerations, and personalization for individual users with different abilities.",expectedOutcome:"Students will understand assistive robotics requirements and be able to design appropriate VLA systems for accessibility applications.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-18-vla-hardware-integration"},"Exercise 18: VLA Hardware Integration"),(0,a.yg)(s.A,{title:"VLA System Hardware Integration",difficulty:"advanced",type:"practical",instructions:"Design and implement the hardware integration for a VLA system including cameras, processors, robotic arms, and safety systems. Consider computational requirements, sensor placement, and real-time constraints.",expectedOutcome:"Students will understand hardware-software integration for VLA systems and be able to design appropriate hardware configurations.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-19-vla-in-dynamic-environments"},"Exercise 19: VLA in Dynamic Environments"),(0,a.yg)(s.A,{title:"VLA Operation in Dynamic Environments",difficulty:"advanced",type:"simulation",instructions:"Implement a VLA system that can operate effectively in dynamic environments where objects and conditions change during task execution. Include online adaptation and replanning capabilities.",expectedOutcome:"Students will understand dynamic environment challenges and be able to implement adaptive VLA systems.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"exercise-20-vla-explainability"},"Exercise 20: VLA Explainability"),(0,a.yg)(s.A,{title:"Explainable VLA Systems",difficulty:"intermediate",type:"theoretical",instructions:"Design mechanisms for explaining VLA system decisions to human users. Include visualization of attention mechanisms, decision pathways, and confidence measures. Create interfaces that make VLA behavior transparent and understandable.",expectedOutcome:"Students will understand explainability in VLA systems and be able to implement transparent AI interfaces.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"programming-exercise-1-simple-vla-pipeline"},"Programming Exercise 1: Simple VLA Pipeline"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass SimpleVLA(nn.Module):\n    """\n    A simplified Vision-Language-Action model for educational purposes.\n    This model demonstrates the basic components of a VLA system.\n    """\n    def __init__(self, vocab_size=10000, hidden_dim=512, action_dim=4):\n        super(SimpleVLA, self).__init__()\n\n        # Visual encoder\n        self.visual_encoder = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Language encoder (simplified embedding model)\n        self.lang_encoder = nn.Embedding(vocab_size, hidden_dim)\n        self.lang_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n\n        # Fusion layer\n        self.fusion = nn.Linear(hidden_dim * 2, hidden_dim)\n\n        # Action decoder\n        self.action_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, images, language_ids):\n        """\n        Forward pass of the VLA model.\n\n        Args:\n            images: Batch of images [B, C, H, W]\n            language_ids: Batch of language token IDs [B, T]\n\n        Returns:\n            actions: Predicted actions [B, action_dim]\n        """\n        # Encode visual input\n        visual_features = self.visual_encoder(images)\n\n        # Encode language input\n        lang_embeddings = self.lang_encoder(language_ids)\n        lang_features, _ = self.lang_lstm(lang_embeddings)\n        # Use the last token\'s features as the sentence representation\n        lang_features = lang_features[:, -1, :]\n\n        # Fuse visual and language features\n        fused_features = torch.cat([visual_features, lang_features], dim=1)\n        fused_features = torch.relu(self.fusion(fused_features))\n\n        # Generate action\n        actions = self.action_head(fused_features)\n\n        return actions\n\ndef train_simple_vla():\n    """\n    Training loop for the simple VLA model.\n    This is a conceptual implementation showing the training process.\n    """\n    # Initialize model\n    model = SimpleVLA()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    criterion = nn.MSELoss()\n\n    # Training loop (conceptual)\n    for epoch in range(100):\n        # Load batch of (images, language, actions)\n        # images = ...  # Visual observations\n        # language = ...  # Language instructions\n        # actions = ...  # Ground truth actions\n\n        # Forward pass\n        # pred_actions = model(images, language)\n        # loss = criterion(pred_actions, actions)\n\n        # Backward pass\n        # optimizer.zero_grad()\n        # loss.backward()\n        # optimizer.step()\n\n        pass  # Implementation would go here\n\n    print("Simple VLA training completed")\n\n# TODO: Implement the actual training loop with real data\n# The above is a framework showing the structure\n')),(0,a.yg)("h2",{id:"programming-exercise-2-visual-grounding-implementation"},"Programming Exercise 2: Visual Grounding Implementation"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import torch\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\n\ndef compute_attention_map(image_features, language_features):\n    """\n    Compute attention map showing which image regions are relevant\n    for the given language instruction.\n\n    Args:\n        image_features: Features from visual encoder [H, W, D]\n        language_features: Features from language encoder [D]\n\n    Returns:\n        attention_map: Attention weights [H, W]\n    """\n    # Reshape image features to [H*W, D]\n    h, w, d = image_features.shape\n    image_flat = image_features.view(-1, d)\n\n    # Compute similarity between image regions and language\n    similarities = torch.matmul(image_flat, language_features)\n\n    # Reshape back to spatial dimensions\n    attention_map = similarities.view(h, w)\n\n    # Apply softmax to get normalized attention weights\n    attention_map = F.softmax(attention_map.view(-1), dim=0).view(h, w)\n\n    return attention_map\n\ndef visualize_grounding(image, attention_map, top_k=10):\n    """\n    Visualize the visual grounding results by highlighting\n    the most attended regions in the image.\n\n    Args:\n        image: Input image [H, W, 3]\n        attention_map: Attention weights [H, W]\n        top_k: Number of top regions to highlight\n\n    Returns:\n        highlighted_image: Image with attention highlights\n    """\n    # Convert attention map to numpy for visualization\n    att_np = attention_map.detach().cpu().numpy()\n\n    # Get top-k attention locations\n    flat_indices = np.argpartition(att_np.flatten(), -top_k)[-top_k:]\n    coords = np.unravel_index(flat_indices, att_np.shape)\n\n    # Create highlighted image\n    highlighted = image.copy()\n\n    for y, x in zip(coords[0], coords[1]):\n        # Draw circle at attention location\n        cv2.circle(highlighted, (x, y), 10, (0, 255, 0), 2)\n\n    return highlighted\n\n# TODO: Implement complete visual grounding system\n# This framework shows the core concepts\n')),(0,a.yg)("h2",{id:"project-exercise-vla-system-for-simple-manipulation"},"Project Exercise: VLA System for Simple Manipulation"),(0,a.yg)(s.A,{title:"Complete VLA System Implementation",difficulty:"advanced",type:"project",instructions:"Implement a complete VLA system for a simple manipulation task in simulation (e.g., moving colored blocks based on language instructions). The system should include visual processing, language understanding, action generation, and evaluation components. Test the system on various tasks and evaluate its performance.",expectedOutcome:"Students will understand the complete implementation of VLA systems and be able to build functional VLA robots for specific tasks.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"research-exercise-vla-architecture-innovation"},"Research Exercise: VLA Architecture Innovation"),(0,a.yg)(s.A,{title:"VLA Architecture Research Project",difficulty:"advanced",type:"research",instructions:"Research and propose an innovative VLA architecture that addresses current limitations (e.g., computational efficiency, generalization, safety). Implement a proof-of-concept and evaluate its advantages over existing approaches. Write a technical report describing your architecture and its benefits.",expectedOutcome:"Students will understand VLA research challenges and be able to propose and evaluate novel architectural approaches.",mdxType:"Exercise"}),(0,a.yg)("h2",{id:"summary"},"Summary"),(0,a.yg)("p",null,"These exercises provide hands-on experience with Vision-Language-Action systems, from basic concepts to advanced implementations. Students should work through these exercises to gain practical understanding of VLA development, evaluation, and deployment considerations."))}u.isMDXComponent=!0},5680:(e,n,i)=>{i.d(n,{xA:()=>d,yg:()=>g});var t=i(6540);function a(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function s(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),i.push.apply(i,t)}return i}function r(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?s(Object(i),!0).forEach(function(n){a(e,n,i[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):s(Object(i)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))})}return e}function o(e,n){if(null==e)return{};var i,t,a=function(e,n){if(null==e)return{};var i,t,a={},s=Object.keys(e);for(t=0;t<s.length;t++)i=s[t],n.indexOf(i)>=0||(a[i]=e[i]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)i=s[t],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var c=t.createContext({}),l=function(e){var n=t.useContext(c),i=n;return e&&(i="function"==typeof e?e(n):r(r({},n),e)),i},d=function(e){var n=l(e.components);return t.createElement(c.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},p=t.forwardRef(function(e,n){var i=e.components,a=e.mdxType,s=e.originalType,c=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=l(i),p=a,g=m["".concat(c,".").concat(p)]||m[p]||u[p]||s;return i?t.createElement(g,r(r({ref:n},d),{},{components:i})):t.createElement(g,r({ref:n},d))});function g(e,n){var i=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var s=i.length,r=new Array(s);r[0]=p;var o={};for(var c in n)hasOwnProperty.call(n,c)&&(o[c]=n[c]);o.originalType=e,o[m]="string"==typeof e?e:a,r[1]=o;for(var l=2;l<s;l++)r[l]=i[l];return t.createElement.apply(null,r)}return t.createElement.apply(null,i)}p.displayName="MDXCreateElement"},9591:(e,n,i)=>{i.d(n,{A:()=>r});var t=i(6540),a=i(53);const s={container:"container_DJUV",header:"header_8Po6",title:"title_gLrK",meta:"meta_upBu",difficulty:"difficulty_mbNi",beginner:"beginner_8ATc",intermediate:"intermediate_FNzA",advanced:"advanced_-ilr",type:"type_bvka",content:"content_LRG-",sectionTitle:"sectionTitle_QGya",instructions:"instructions_d92c",expectedOutcome:"expectedOutcome_FOyF"},r=({title:e,difficulty:n,type:i,instructions:r,expectedOutcome:o})=>t.createElement("div",{className:(0,a.A)("exercise-container",s.container)},t.createElement("div",{className:s.header},t.createElement("h3",{className:s.title},e||"Exercise"),t.createElement("div",{className:s.meta},t.createElement("span",{className:(0,a.A)(s.difficulty,(e=>{switch(e?.toLowerCase()){case"beginner":return s.beginner;case"intermediate":return s.intermediate;case"advanced":return s.advanced;default:return s.default}})(n))},n||"Unknown Level"),t.createElement("span",{className:s.type},(e=>{switch(e?.toLowerCase()){case"theoretical":return"Theoretical";case"practical":return"Practical";case"simulation":return"Simulation";case"hardware":return"Hardware";default:return e||"Exercise"}})(i)))),t.createElement("div",{className:s.content},t.createElement("h4",{className:s.sectionTitle},"Instructions:"),t.createElement("div",{className:s.instructions},r?t.createElement("div",{dangerouslySetInnerHTML:{__html:r}}):t.createElement("p",null,"No instructions provided.")),o&&t.createElement(t.Fragment,null,t.createElement("h4",{className:s.sectionTitle},"Expected Outcome:"),t.createElement("div",{className:s.expectedOutcome},o))))}}]);